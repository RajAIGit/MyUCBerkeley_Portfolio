{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical Application III: Comparing Classifiers\n",
    "\n",
    "**Overview**: In this practical application, your goal is to compare the performance of the classifiers we encountered in this section, namely K Nearest Neighbor, Logistic Regression, Decision Trees, and Support Vector Machines.  We will utilize a dataset related to marketing bank products over the telephone.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started\n",
    "\n",
    "Our dataset comes from the UCI Machine Learning repository [link](https://archive.ics.uci.edu/ml/datasets/bank+marketing).  The data is from a Portugese banking institution and is a collection of the results of multiple marketing campaigns.  We will make use of the article accompanying the dataset [here](CRISP-DM-BANK.pdf) for more information on the data and features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 1: Understanding the Data\n",
    "\n",
    "To gain a better understanding of the data, please read the information provided in the UCI link above, and examine the **Materials and Methods** section of the paper.  How many marketing campaigns does this data represent?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marketing Campaigns:\n",
    "\n",
    "There are two main approaches for enterprises to promote products and/or services: through mass campaigns,\n",
    "targeting general indiscriminate public or directedmarketing, targeting a specific set of contacts.\n",
    "Nowadays, in a global competitive world, positive responses to mass campaigns are typically very low, \n",
    "less than 1%, according to the same study. Alternatively, directed marketing focus on targets that assumable will be\n",
    "keener to that specific product/service, making this kind of campaigns more attractive due to its efficiency.\n",
    "Nevertheless, directed marketing has some drawbacks, for instance it may trigger a negative attitude\n",
    "towards banks due to the intrusion of privacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 2: Read in the Data\n",
    "\n",
    "Use pandas to read in the dataset `bank-additional-full.csv` and assign to a meaningful variable name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/bank-additional-full.csv', sep = ';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>job</th>\n",
       "      <th>marital</th>\n",
       "      <th>education</th>\n",
       "      <th>default</th>\n",
       "      <th>housing</th>\n",
       "      <th>loan</th>\n",
       "      <th>contact</th>\n",
       "      <th>month</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>...</th>\n",
       "      <th>campaign</th>\n",
       "      <th>pdays</th>\n",
       "      <th>previous</th>\n",
       "      <th>poutcome</th>\n",
       "      <th>emp.var.rate</th>\n",
       "      <th>cons.price.idx</th>\n",
       "      <th>cons.conf.idx</th>\n",
       "      <th>euribor3m</th>\n",
       "      <th>nr.employed</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>56</td>\n",
       "      <td>housemaid</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.4y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>unknown</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>37</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>40</td>\n",
       "      <td>admin.</td>\n",
       "      <td>married</td>\n",
       "      <td>basic.6y</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56</td>\n",
       "      <td>services</td>\n",
       "      <td>married</td>\n",
       "      <td>high.school</td>\n",
       "      <td>no</td>\n",
       "      <td>no</td>\n",
       "      <td>yes</td>\n",
       "      <td>telephone</td>\n",
       "      <td>may</td>\n",
       "      <td>mon</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>999</td>\n",
       "      <td>0</td>\n",
       "      <td>nonexistent</td>\n",
       "      <td>1.1</td>\n",
       "      <td>93.994</td>\n",
       "      <td>-36.4</td>\n",
       "      <td>4.857</td>\n",
       "      <td>5191.0</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   age        job  marital    education  default housing loan    contact  \\\n",
       "0   56  housemaid  married     basic.4y       no      no   no  telephone   \n",
       "1   57   services  married  high.school  unknown      no   no  telephone   \n",
       "2   37   services  married  high.school       no     yes   no  telephone   \n",
       "3   40     admin.  married     basic.6y       no      no   no  telephone   \n",
       "4   56   services  married  high.school       no      no  yes  telephone   \n",
       "\n",
       "  month day_of_week  ...  campaign  pdays  previous     poutcome emp.var.rate  \\\n",
       "0   may         mon  ...         1    999         0  nonexistent          1.1   \n",
       "1   may         mon  ...         1    999         0  nonexistent          1.1   \n",
       "2   may         mon  ...         1    999         0  nonexistent          1.1   \n",
       "3   may         mon  ...         1    999         0  nonexistent          1.1   \n",
       "4   may         mon  ...         1    999         0  nonexistent          1.1   \n",
       "\n",
       "   cons.price.idx  cons.conf.idx  euribor3m  nr.employed   y  \n",
       "0          93.994          -36.4      4.857       5191.0  no  \n",
       "1          93.994          -36.4      4.857       5191.0  no  \n",
       "2          93.994          -36.4      4.857       5191.0  no  \n",
       "3          93.994          -36.4      4.857       5191.0  no  \n",
       "4          93.994          -36.4      4.857       5191.0  no  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 3: Understanding the Features\n",
    "\n",
    "\n",
    "Examine the data description below, and determine if any of the features are missing values or need to be coerced to a different data type.\n",
    "\n",
    "\n",
    "```\n",
    "Input variables:\n",
    "# bank client data:\n",
    "1 - age (numeric)\n",
    "2 - job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\n",
    "3 - marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n",
    "4 - education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')\n",
    "5 - default: has credit in default? (categorical: 'no','yes','unknown')\n",
    "6 - housing: has housing loan? (categorical: 'no','yes','unknown')\n",
    "7 - loan: has personal loan? (categorical: 'no','yes','unknown')\n",
    "# related with the last contact of the current campaign:\n",
    "8 - contact: contact communication type (categorical: 'cellular','telephone')\n",
    "9 - month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n",
    "10 - day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')\n",
    "11 - duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n",
    "# other attributes:\n",
    "12 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n",
    "13 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n",
    "14 - previous: number of contacts performed before this campaign and for this client (numeric)\n",
    "15 - poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')\n",
    "# social and economic context attributes\n",
    "16 - emp.var.rate: employment variation rate - quarterly indicator (numeric)\n",
    "17 - cons.price.idx: consumer price index - monthly indicator (numeric)\n",
    "18 - cons.conf.idx: consumer confidence index - monthly indicator (numeric)\n",
    "19 - euribor3m: euribor 3 month rate - daily indicator (numeric)\n",
    "20 - nr.employed: number of employees - quarterly indicator (numeric)\n",
    "\n",
    "Output variable (desired target):\n",
    "21 - y - has the client subscribed a term deposit? (binary: 'yes','no')\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Fields with implicit missing values\n",
    "\n",
    "These columns encode “missing/unknown” using strings or sentinel values and should be handled explicitly:\n",
    "\t• job, marital, education, default, housing, loan\n",
    "\t\t○ Contain 'unknown' → this is a missing value placeholder.\n",
    "\t\t○ Recommended: convert 'unknown' to NaN (or keep as a separate category if you want to model “unknown” as informative).\n",
    "\t• pdays\n",
    "\t\t○ 999 means “not previously contacted” → this is a sentinel for “not applicable / unknown.”\n",
    "\t\t○ Recommended: set pdays = NaN when pdays == 999, and add a boolean flag (e.g., never_contacted_before = (pdays == 999) before coercion) so you don’t lose signal.\n",
    "\t• poutcome\n",
    "\t\t○ Values: 'failure', 'success', 'nonexistent'.\n",
    "\t\t○ 'nonexistent' means no prior campaign (not strictly missing), but it should be consistent with previous == 0 and pdays == 999.\n",
    "\t\t○ Recommended: keep as categorical, but validate consistency (see §3).\n",
    "\n",
    "2) Fields that should be categorical (nominal/ordered)\n",
    "\n",
    "Convert these from strings to category (with ordering where appropriate):\n",
    "\t• Nominal categorical:\n",
    "\t\t○ job, marital, education, default, housing, loan, contact, poutcome, y\n",
    "\t\t○ Note on y: convert to binary target (1 = 'yes', 0 = 'no') for modeling, keeping original as a label if desired.\n",
    "\t• Ordered categorical:\n",
    "\t\t○ month: ['jan','feb','mar','apr','may','jun','jul','aug','sep','oct','nov','dec']\n",
    "\t\t○ day_of_week: ['mon','tue','wed','thu','fri']\n",
    "\t\t○ (Optional) education: you may impose an order if analytically justified:\\ illiterate < basic.4y < basic.6y < basic.9y < high.school < professional.course < university.degree\\ Be cautious—this imposes an ordinal structure that may not be universally appropriate.\n",
    "\n",
    "3) Fields that should be numeric (and likely integer vs float)\n",
    "Make sure these are correctly typed:\n",
    "\t• Integer (count-like):\n",
    "\t\t○ age, duration, campaign, previous, pdays\n",
    "\t\t○ Notes:\n",
    "\t\t\t§ duration is numeric but must not be used for realistic predictive modeling (it “leaks” outcome; use only for benchmarking as per the note).\n",
    "\t\t\t§ After handling pdays == 999 → NaN, you’ll typically store pdays as float (because of the NaNs) or keep as integer with a pandas nullable integer dtype.\n",
    "\t• Float (economic indicators):\n",
    "\t\t○ emp.var.rate, cons.price.idx, cons.conf.idx, euribor3m, nr.employed\n",
    "\t\t○ These are continuous and should be float.\n",
    "\n",
    "4) Cross-field consistency & sanity checks\n",
    "Add these validations to catch data issues:\n",
    "\t• Previous contact coherence\n",
    "\t\t○ If previous == 0 → expect poutcome == 'nonexistent' and pdays == 999.\n",
    "\t\t○ If previous > 0 → expect poutcome ∈ {'failure','success'} and pdays < 999.\n",
    "\t• Bounds & domain checks\n",
    "\t\t○ campaign >= 1\n",
    "\t\t○ previous >= 0\n",
    "\t\t○ duration >= 0 (0 occurs and is valid; just note its strong correlation with y='no')\n",
    "\t\t○ pdays == 999 or pdays >= 0\n",
    "\t\t○ month ∈ {'jan',...,'dec'}\n",
    "\t\t○ day_of_week ∈ {'mon','tue','wed','thu','fri'}\n",
    "\t• Unknowns\n",
    "\t\t○ Review the incidence of 'unknown' in job, marital, education, default, housing, loan. If high, consider:\n",
    "\t\t\t§ Keeping 'unknown' as its own informative category, or\n",
    "\t\t\t§ Imputing / treating as NaN depending on your modeling strategy.\n",
    "\n",
    "5) Recommended target and leakage handling\n",
    "\t• y (target): Convert to binary (1 for 'yes', 0 for 'no'), keep original label column for interpretability if useful.\n",
    "\t• duration: Exclude from training features for any realistic predictive setting (include only for benchmarking).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 4: Understanding the Task\n",
    "\n",
    "After examining the description and data, your goal now is to clearly state the *Business Objective* of the task.  State the objective below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41188 entries, 0 to 41187\n",
      "Data columns (total 21 columns):\n",
      " #   Column          Non-Null Count  Dtype  \n",
      "---  ------          --------------  -----  \n",
      " 0   age             41188 non-null  int64  \n",
      " 1   job             41188 non-null  object \n",
      " 2   marital         41188 non-null  object \n",
      " 3   education       41188 non-null  object \n",
      " 4   default         41188 non-null  object \n",
      " 5   housing         41188 non-null  object \n",
      " 6   loan            41188 non-null  object \n",
      " 7   contact         41188 non-null  object \n",
      " 8   month           41188 non-null  object \n",
      " 9   day_of_week     41188 non-null  object \n",
      " 10  duration        41188 non-null  int64  \n",
      " 11  campaign        41188 non-null  int64  \n",
      " 12  pdays           41188 non-null  int64  \n",
      " 13  previous        41188 non-null  int64  \n",
      " 14  poutcome        41188 non-null  object \n",
      " 15  emp.var.rate    41188 non-null  float64\n",
      " 16  cons.price.idx  41188 non-null  float64\n",
      " 17  cons.conf.idx   41188 non-null  float64\n",
      " 18  euribor3m       41188 non-null  float64\n",
      " 19  nr.employed     41188 non-null  float64\n",
      " 20  y               41188 non-null  object \n",
      "dtypes: float64(5), int64(5), object(11)\n",
      "memory usage: 6.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Business Objective:\n",
    "\n",
    "Goal was to increase efficiency of directed campaigns for long-term depositsubscriptions by reducing the number of contacts to do.\n",
    "During the Data Understanding phase, we analyzed the data main characteristics. \n",
    "The output presented in the reports of previous campaigns was composed of two values: the result and the amount of money invested.  \n",
    "For this research, only the nominal result was accounted for, thus the goal is to predict if a client will subscribe the deposit.\n",
    "Not regarding which amount is retained, turning it a classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 5: Engineering Features\n",
    "\n",
    "Now that you understand your business objective, we will build a basic model to get started.  Before we can do this, we must work to encode the data.  Using just the bank information features, prepare the features and target column for modeling with appropriate encoding and transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# 1) Columns in scope (bank client data only)\n",
    "bank_num = ['age']\n",
    "bank_cat = ['job', 'marital', 'education', 'default', 'housing', 'loan']\n",
    "\n",
    "# 2) Basic coercions (protect against accidental string types)\n",
    "def coerce_and_clean_bank_info(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    # Coerce numeric\n",
    "    df['age'] = pd.to_numeric(df['age'], errors='coerce')\n",
    "    # Ensure categoricals are strings (so encoders behave consistently)\n",
    "    for col in bank_cat + ['y']:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype('string')\n",
    "    return df\n",
    "\n",
    "# 3) Build preprocessing for numeric & categorical\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    # (No scaler here; age in raw units is fine for tree models.\n",
    "    # If you plan to use linear/logistic models, consider adding StandardScaler.)\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    # If you want to treat 'unknown' as missing, uncomment the next line and the replace step below\n",
    "    # ('unknown_to_nan', FunctionTransformer(lambda X: X.replace('unknown', np.nan)))\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),  # handles NaN if present\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, bank_num),\n",
    "        ('cat', categorical_transformer, bank_cat),\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# 4) Prepare X (features) and y (target)\n",
    "def prepare_features_and_target(df_raw: pd.DataFrame):\n",
    "    df = coerce_and_clean_bank_info(df_raw)\n",
    "\n",
    "    # Map target y to binary 1/0\n",
    "    if 'y' not in df.columns:\n",
    "        raise KeyError(\"Target column 'y' not found in DataFrame.\")\n",
    "    y = df['y'].map({'yes': 1, 'no': 0}).astype('Int64')\n",
    "\n",
    "    # Features: only bank info\n",
    "    X = df[bank_num + bank_cat]\n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 6: Train/Test Split\n",
    "\n",
    "With your data prepared, split it into a train and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Define columns in scope\n",
    "# -----------------------------\n",
    "BANK_NUM = ['age']\n",
    "BANK_CAT = ['job', 'marital', 'education', 'default', 'housing', 'loan']\n",
    "BANK_INFO_COLS = BANK_NUM + BANK_CAT\n",
    "TARGET_COL = 'y'\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Coercion & preparation\n",
    "# -----------------------------\n",
    "def coerce_and_prepare_bank_info(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    - Coerces dtypes for bank info features\n",
    "    - Maps y to binary (1='yes', 0='no')\n",
    "    - Returns X (features) and y (Series)\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "\n",
    "    # Ensure required columns exist\n",
    "    missing = set(BANK_INFO_COLS + [TARGET_COL]) - set(df.columns)\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing required columns: {sorted(missing)}\")\n",
    "\n",
    "    # Numeric coercion\n",
    "    df['age'] = pd.to_numeric(df['age'], errors='coerce')\n",
    "\n",
    "    # Keep categorical as string (robust for downstream encoders)\n",
    "    for col in BANK_CAT + [TARGET_COL]:\n",
    "        df[col] = df[col].astype('string')\n",
    "\n",
    "    # Target mapping\n",
    "    y = df[TARGET_COL].map({'yes': 1, 'no': 0})\n",
    "    if y.isna().any():\n",
    "        # If any unexpected labels exist, raise a helpful error\n",
    "        bad_labels = df.loc[y.isna(), TARGET_COL].unique().tolist()\n",
    "        raise ValueError(f\"Target y contains unexpected labels: {bad_labels}. Expected 'yes'/'no'.\")\n",
    "\n",
    "    # Features only from bank info\n",
    "    X = df[BANK_INFO_COLS].copy()\n",
    "\n",
    "    return X, y.astype('int64')\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Stratified train/test split\n",
    "# -----------------------------\n",
    "def split_train_test_bank_info(df: pd.DataFrame,\n",
    "                               test_size: float = 0.2,\n",
    "                               random_state: int = 42):\n",
    "    \"\"\"\n",
    "    Returns: X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    X, y = coerce_and_prepare_bank_info(df)\n",
    "\n",
    "    # Stratify on y to preserve class distribution across splits\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y,\n",
    "        test_size=test_size,\n",
    "        random_state=random_state,\n",
    "        stratify=y\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 7: A Baseline Model\n",
    "\n",
    "Before we build our first model, we want to establish a baseline.  What is the baseline performance that our classifier should aim to beat?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, balanced_accuracy_score, f1_score,\n",
    "    roc_auc_score, average_precision_score, log_loss, brier_score_loss\n",
    ")\n",
    "\n",
    "# Assume you already have y_train, y_test as 0/1 (1='yes')\n",
    "def compute_baselines(y_true: pd.Series, k_frac: float = 0.1):\n",
    "    \"\"\"\n",
    "    Compute baseline metrics for classification/ranking.\n",
    "    k_frac: fraction of the test population you plan to contact (e.g., top 10%).\n",
    "    \"\"\"\n",
    "    y_true = pd.Series(y_true).astype(int)\n",
    "    n = len(y_true)\n",
    "    p = y_true.mean()  # prevalence\n",
    "\n",
    "    # --- Majority-class classifier: predict all 0 (no)\n",
    "    y_pred_majority = np.zeros(n, dtype=int)\n",
    "    majority_acc = accuracy_score(y_true, y_pred_majority)\n",
    "    majority_bal_acc = balanced_accuracy_score(y_true, y_pred_majority)\n",
    "    majority_f1_pos = f1_score(y_true, y_pred_majority, zero_division=0)\n",
    "\n",
    "    # --- Random/constant scores for threshold-free metrics\n",
    "    # Use constant probability p (best constant in expectation)\n",
    "    y_score_const = np.full(n, fill_value=p, dtype=float)\n",
    "\n",
    "    # ROC AUC baseline (for random, it’s 0.5; many libs will error on constant scores; guard it)\n",
    "    roc_auc_base = 0.5\n",
    "\n",
    "    # PR AUC baseline equals prevalence\n",
    "    pr_auc_base = p\n",
    "\n",
    "    # Log loss baseline at constant p\n",
    "    log_loss_base = log_loss(y_true, y_score_const, labels=[0,1])\n",
    "\n",
    "    # Brier score baseline at constant p\n",
    "    brier_base = brier_score_loss(y_true, y_score_const)\n",
    "\n",
    "    # --- Ranking baselines for top-k targeting\n",
    "    k = max(1, int(np.round(k_frac * n)))\n",
    "    # Random ranking baseline expects precision@k ~= p, lift@k ~= 1\n",
    "    precision_at_k_base = p\n",
    "    lift_at_k_base = 1.0\n",
    "    # Expected recall@k under random = (k/n)\n",
    "    recall_at_k_base = min(1.0, k / n)\n",
    "\n",
    "    return {\n",
    "        \"prevalence (positive rate)\": p,\n",
    "        \"majority_class_accuracy\": majority_acc,\n",
    "        \"majority_class_balanced_accuracy\": majority_bal_acc,\n",
    "        \"majority_class_F1_positive\": majority_f1_pos,\n",
    "        \"roc_auc_baseline\": roc_auc_base,\n",
    "        \"pr_auc_baseline\": pr_auc_base,\n",
    "        \"log_loss_baseline\": log_loss_base,\n",
    "        \"brier_score_baseline\": brier_base,\n",
    "        f\"precision@{int(k_frac*100)}%_baseline\": precision_at_k_base,\n",
    "        f\"recall@{int(k_frac*100)}%_baseline\": recall_at_k_base,\n",
    "        f\"lift@{int(k_frac*100)}%_baseline\": lift_at_k_base,\n",
    "        \"note\": \"Compute these on y_test (not y_train).\"\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 8: A Simple Model\n",
    "\n",
    "Use Logistic Regression to build a basic model on your data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, balanced_accuracy_score, f1_score, roc_auc_score,\n",
    "    average_precision_score, classification_report, confusion_matrix\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# 0) Configuration\n",
    "# -----------------------------\n",
    "CONTACT_RATE = 0.10     # evaluate precision/recall/lift at top 10% by default\n",
    "TEST_SIZE = 0.20\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# -----------------------------\n",
    "# 1) Define scope: bank info features and target\n",
    "# -----------------------------\n",
    "BANK_NUM = ['age']\n",
    "BANK_CAT = ['job', 'marital', 'education', 'default', 'housing', 'loan']\n",
    "FEATURES = BANK_NUM + BANK_CAT\n",
    "TARGET = 'y'\n",
    "\n",
    "# -----------------------------\n",
    "# 2) Helpers: prepare data\n",
    "# -----------------------------\n",
    "def prepare_bank_features_and_target(df: pd.DataFrame):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Ensure required columns exist\n",
    "    missing = set(FEATURES + [TARGET]) - set(df.columns)\n",
    "    if missing:\n",
    "        raise KeyError(f\"Missing required columns: {sorted(missing)}\")\n",
    "\n",
    "    # Coerce numeric\n",
    "    df['age'] = pd.to_numeric(df['age'], errors='coerce')\n",
    "\n",
    "    # Categoricals to string (robust for OHE)\n",
    "    for col in BANK_CAT + [TARGET]:\n",
    "        df[col] = df[col].astype('string')\n",
    "\n",
    "    # Target mapping\n",
    "    y = df[TARGET].map({'yes': 1, 'no': 0})\n",
    "    if y.isna().any():\n",
    "        bad = df.loc[y.isna(), TARGET].unique().tolist()\n",
    "        raise ValueError(f\"Unexpected target labels: {bad}. Expected 'yes'/'no'.\")\n",
    "\n",
    "    # Features\n",
    "    X = df[FEATURES].copy()\n",
    "    return X, y.astype(int)\n",
    "\n",
    "# -----------------------------\n",
    "# 3) Preprocessing & model\n",
    "# -----------------------------\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median'))\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    # Treat 'unknown' as a valid category; impute missing with most frequent\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, BANK_NUM),\n",
    "        ('cat', categorical_transformer, BANK_CAT),\n",
    "    ],\n",
    "    remainder='drop'\n",
    ")\n",
    "\n",
    "# Logistic Regression baseline\n",
    "logreg = LogisticRegression(\n",
    "    max_iter=1000,\n",
    "    class_weight='balanced',     # handle imbalance\n",
    "    solver='lbfgs',              # reliable for OHE inputs\n",
    "    n_jobs=None,                 # n_jobs not used by lbfgs; leave as default\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "# Full pipeline: preprocessing + model\n",
    "clf = Pipeline(steps=[\n",
    "    ('prep', preprocessor),\n",
    "    ('model', logreg)\n",
    "])\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# 4) Train/test split\n",
    "# -----------------------------\n",
    "def stratified_split(df: pd.DataFrame, test_size=TEST_SIZE, random_state=RANDOM_STATE):\n",
    "    X, y = prepare_bank_features_and_target(df)\n",
    "    return train_test_split(X, y, test_size=test_size, random_state=random_state, stratify=y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 9: Score the Model\n",
    "\n",
    "What is the accuracy of your model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.5849\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = stratified_split(df, test_size=0.2, random_state=42)\n",
    "    \n",
    "# Fit\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "test_accuracy = clf.score(X_test, y_test)  # same as accuracy_score(y_test, clf.predict(X_test))\n",
    "\n",
    "print(f\"Test accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 10: Model Comparisons\n",
    "\n",
    "Now, we aim to compare the performance of the Logistic Regression model to our KNN algorithm, Decision Tree, and SVM models.  Using the default settings for each of the models, fit and score each.  Also, be sure to compare the fit time of each of the models.  Present your findings in a `DataFrame` similar to that below:\n",
    "\n",
    "| Model | Train Time | Train Accuracy | Test Accuracy |\n",
    "| ----- | ---------- | -------------  | -----------   |\n",
    "|     |    |.     |.     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              Model  Train Time (s)  Train Accuracy  Test Accuracy\n",
      "                SVM          4.5336          0.8873         0.8874\n",
      "                KNN          0.0480          0.8914         0.8786\n",
      "      Decision Tree          0.1001          0.9171         0.8648\n",
      "Logistic Regression          0.1534          0.5913         0.5849\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import time\n",
    "\n",
    "# -----------------------------\n",
    "# Models (default settings)\n",
    "# -----------------------------\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, class_weight='balanced', random_state=RANDOM_STATE),\n",
    "    'KNN': KNeighborsClassifier(),                   # n_neighbors=5 by default\n",
    "    'Decision Tree': DecisionTreeClassifier(random_state=RANDOM_STATE),\n",
    "    'SVM': SVC()                                     # default kernel='rbf'\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# Benchmark function\n",
    "# -----------------------------\n",
    "def fit_and_score_models(df: pd.DataFrame):\n",
    "    X, y = prepare_bank_features_and_target(df)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
    "    )\n",
    "\n",
    "    rows = []\n",
    "    for name, base_model in models.items():\n",
    "        clf = Pipeline(steps=[\n",
    "            ('prep', preprocessor),\n",
    "            ('model', base_model)\n",
    "        ])\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        clf.fit(X_train, y_train)\n",
    "        t1 = time.perf_counter()\n",
    "        train_time = t1 - t0\n",
    "\n",
    "        # Train accuracy\n",
    "        y_pred_train = clf.predict(X_train)\n",
    "        train_acc = accuracy_score(y_train, y_pred_train)\n",
    "\n",
    "        # Test accuracy\n",
    "        y_pred_test = clf.predict(X_test)\n",
    "        test_acc = accuracy_score(y_test, y_pred_test)\n",
    "\n",
    "        rows.append({\n",
    "            'Model': name,\n",
    "            'Train Time (s)': round(train_time, 4),\n",
    "            'Train Accuracy': round(train_acc, 4),\n",
    "            'Test Accuracy': round(test_acc, 4),\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(rows).sort_values('Test Accuracy', ascending=False).reset_index(drop=True)\n",
    "    return results_df\n",
    "\n",
    "# -----------------------------\n",
    "# 6) Run benchmark\n",
    "# -----------------------------\n",
    "results = fit_and_score_models(df)\n",
    "print(results.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem 11: Improving the Model\n",
    "\n",
    "Now that we have some basic models on the board, we want to try to improve these.  Below, we list a few things to explore in this pursuit.\n",
    "\n",
    "\n",
    "- Hyperparameter tuning and grid search.  All of our models have additional hyperparameters to tune and explore.  For example the number of neighbors in KNN or the maximum depth of a Decision Tree.  \n",
    "- Adjust your performance metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function ResourceTracker.__del__ at 0x1068a5bc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x1078adbc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x105c1dbc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x123d3dbc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n",
      "Exception ignored in: <function ResourceTracker.__del__ at 0x106df1bc0>\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 82, in __del__\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 91, in _stop\n",
      "  File \"/opt/anaconda3/lib/python3.13/multiprocessing/resource_tracker.py\", line 116, in _stop_locked\n",
      "ChildProcessError: [Errno 10] No child processes\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, average_precision_score, roc_auc_score, balanced_accuracy_score, f1_score, make_scorer\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Define models and search spaces\n",
    "# -----------------------------\n",
    "base_lr = LogisticRegression(max_iter=1000, class_weight='balanced', solver='lbfgs', random_state=RANDOM_STATE)\n",
    "base_knn = KNeighborsClassifier()\n",
    "base_dt = DecisionTreeClassifier(random_state=RANDOM_STATE)\n",
    "base_svm = SVC(probability=True, random_state=RANDOM_STATE)  # probability True for PR/precision@k\n",
    "\n",
    "pipelines = {\n",
    "    \"Logistic Regression\": Pipeline([('prep', preprocessor), ('model', base_lr)]),\n",
    "    \"KNN\": Pipeline([('prep', preprocessor), ('model', base_knn)]),\n",
    "    \"Decision Tree\": Pipeline([('prep', preprocessor), ('model', base_dt)]),\n",
    "    \"SVM\": Pipeline([('prep', preprocessor), ('model', base_svm)]),\n",
    "}\n",
    "\n",
    "param_grids = {\n",
    "    \"Logistic Regression\": {\n",
    "        'model__C': [0.1, 0.5, 1.0, 2.0, 5.0],\n",
    "        'model__penalty': ['l2'],\n",
    "        # 'model__solver': ['lbfgs', 'liblinear']  # liblinear for smaller problems; keep lbfgs for OHE\n",
    "    },\n",
    "    \"KNN\": {\n",
    "        'model__n_neighbors': [3, 5, 7, 11, 21],\n",
    "        'model__weights': ['uniform', 'distance'],\n",
    "        'model__metric': ['euclidean', 'manhattan']\n",
    "    },\n",
    "    \"Decision Tree\": {\n",
    "        'model__max_depth': [None, 5, 10, 20, 30],\n",
    "        'model__min_samples_split': [2, 10, 50],\n",
    "        'model__min_samples_leaf': [1, 5, 10, 20],\n",
    "        'model__ccp_alpha': [0.0, 0.001, 0.01],\n",
    "        'model__class_weight': [None, 'balanced']\n",
    "    },\n",
    "    \"SVM\": {\n",
    "        'model__kernel': ['rbf'],\n",
    "        'model__C': [0.5, 1.0, 2.0, 5.0],\n",
    "        'model__gamma': ['scale', 0.1, 0.01]\n",
    "    }\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# Run searches and evaluate\n",
    "# -----------------------------\n",
    "def tune_and_compare(df: pd.DataFrame, primary_scorer='average_precision'):\n",
    "    \"\"\"\n",
    "    primary_scorer: 'average_precision' (PR AUC), 'roc_auc', or use precision_at_k_scorer\n",
    "    \"\"\"\n",
    "    if primary_scorer == 'precision_at_k':\n",
    "        scorer = precision_at_k_scorer\n",
    "    else:\n",
    "        scorer = primary_scorer  # string accepted by sklearn scorers\n",
    "\n",
    "    X, y = prepare_bank_features_and_target(df)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y\n",
    "    )\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "    rows = []\n",
    "    best_estimators = {}\n",
    "\n",
    "    for name, pipe in pipelines.items():\n",
    "        grid = param_grids[name]\n",
    "        search = GridSearchCV(\n",
    "            estimator=pipe,\n",
    "            param_grid=grid,\n",
    "            scoring=scorer,\n",
    "            cv=cv,\n",
    "            n_jobs=-1,\n",
    "            refit=True,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        search.fit(X_train, y_train)\n",
    "        t1 = time.perf_counter()\n",
    "\n",
    "        best_estimators[name] = search.best_estimator_\n",
    "        train_time = t1 - t0\n",
    "\n",
    "        # Evaluate on held-out test\n",
    "        best_model = search.best_estimator_\n",
    "\n",
    "        # Predict probabilities if available\n",
    "        if hasattr(best_model.named_steps['model'], 'predict_proba'):\n",
    "            y_proba_test = best_model.predict_proba(X_test)[:, 1]\n",
    "        elif hasattr(best_model.named_steps['model'], 'decision_function'):\n",
    "            # decision_function may output uncalibrated scores; still OK for ranking\n",
    "            y_proba_test = best_model.decision_function(X_test)\n",
    "        else:\n",
    "            y_proba_test = best_model.predict(X_test)\n",
    "\n",
    "        y_pred_test = (y_proba_test >= 0.5).astype(int) if y_proba_test.ndim == 1 else best_model.predict(X_test)\n",
    "        y_pred_train = best_model.predict(X_train)\n",
    "\n",
    "        # Metrics\n",
    "        test_acc = accuracy_score(y_test, y_pred_test)\n",
    "        train_acc = accuracy_score(y_train, y_pred_train)\n",
    "        roc = roc_auc_score(y_test, y_proba_test) if len(np.unique(y_test)) == 2 else np.nan\n",
    "        pr_auc = average_precision_score(y_test, y_proba_test)\n",
    "        bal_acc = balanced_accuracy_score(y_test, y_pred_test)\n",
    "        f1_pos = f1_score(y_test, y_pred_test, zero_division=0)\n",
    "\n",
    "        # Top-k metrics\n",
    "        n_test = len(y_test)\n",
    "        k = max(1, int(round(CONTACT_RATE * n_test)))\n",
    "        order = np.argsort(-y_proba_test)\n",
    "        topk = order[:k]\n",
    "        precision_at_k = y_test.to_numpy()[topk].sum() / k\n",
    "        prevalence = y_test.mean()\n",
    "        lift_at_k = (precision_at_k / prevalence) if prevalence > 0 else np.nan\n",
    "\n",
    "        rows.append({\n",
    "            'Model': name,\n",
    "            'Best Params': search.best_params_,\n",
    "            'CV Best Score (primary)': search.best_score_,\n",
    "            'Train Time (s)': round(train_time, 3),\n",
    "            'Train Accuracy': round(train_acc, 4),\n",
    "            'Test Accuracy': round(test_acc, 4),\n",
    "            'ROC AUC': round(roc, 4),\n",
    "            'PR AUC': round(pr_auc, 4),\n",
    "            f'Precision@{int(CONTACT_RATE*100)}%': round(precision_at_k, 4),\n",
    "            f'Lift@{int(CONTACT_RATE*100)}%': round(lift_at_k, 2),\n",
    "            'Balanced Acc': round(bal_acc, 4),\n",
    "            'F1 (pos)': round(f1_pos, 4),\n",
    "        })\n",
    "\n",
    "    results_df = pd.DataFrame(rows).sort_values(['PR AUC', f'Precision@{int(CONTACT_RATE*100)}%'], ascending=False)\n",
    "    return results_df, best_estimators\n",
    "\n",
    "# -----------------------------\n",
    "# Example usage:\n",
    "# -----------------------------\n",
    "results_pr, best_models_pr = tune_and_compare(df, primary_scorer='average_precision')  # PR AUC\n",
    "print(\"=== Model Comparison (PR AUC as primary) ===\")\n",
    "print(results_pr.to_string(index=False))\n",
    "#\n",
    "results_patk, best_models_patk = tune_and_compare(df, primary_scorer='precision_at_k')  # Precision@k\n",
    "print(\"\\n=== Model Comparison (Precision@k as primary) ===\")\n",
    "print(results_patk.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Questions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
